import base64
import json
import os
import traceback
from datetime import datetime, timezone
from typing import Any, Dict, Optional, Tuple

import functions_framework
from flask import Request

from google.cloud import documentai_v1 as documentai
from google.cloud import firestore
from google.cloud import storage
from google.cloud import pubsub_v1
from google.protobuf.json_format import MessageToDict

# =============================================================================
# Environment (required)
# =============================================================================
PROJECT_ID = os.environ["PROJECT_ID"]
DOCAI_PROCESSOR = os.environ["DOCAI_PROCESSOR"]          # full resource name
DOCAI_RESULTS_TOPIC = os.environ["DOCAI_RESULTS_TOPIC"]  # topic name (not full path)
DOCAI_OUT_BUCKET = os.environ["DOCAI_OUT_BUCKET"]        # bucket name, without gs://

# =============================================================================
# Clients (module-level reuse)
# =============================================================================
storage_client = storage.Client()
fs = firestore.Client(project=PROJECT_ID)
publisher = pubsub_v1.PublisherClient()
topic_path = publisher.topic_path(PROJECT_ID, DOCAI_RESULTS_TOPIC)

# =============================================================================
# Helpers
# =============================================================================

def _utc_ts() -> str:
    return datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")


def _is_pubsub_like_request(request: Request, body: Optional[dict]) -> bool:
    """
    Heuristic: identify Pub/Sub push (or Eventarc-wrapped Pub/Sub) so we can ACK (2xx)
    even on bad payload to avoid infinite retries / poison loops.
    """
    if request.headers.get("ce-type") or request.headers.get("ce-id") or request.headers.get("ce-source"):
        return True
    if isinstance(body, dict):
        if isinstance(body.get("message"), dict):
            return True
        data = body.get("data")
        if isinstance(data, dict) and isinstance(data.get("message"), dict):
            return True
    return False


def _parse_json_body(request: Request) -> Tuple[Optional[Dict[str, Any]], str]:
    """
    Robustly parse JSON from:
      - request.get_json()
      - raw bytes request.data
    Never treat bytes like a dict (fixes 'bytes' object has no attribute 'get').
    """
    body = request.get_json(silent=True)
    if isinstance(body, dict):
        return body, ""

    raw = request.get_data() or b""
    if not raw:
        return None, "Empty request body"

    try:
        parsed = json.loads(raw.decode("utf-8"))
        if isinstance(parsed, dict):
            return parsed, ""
        return None, "JSON body is not an object"
    except Exception:
        return None, "Request body is not valid JSON"


def _unwrap_cloudevent_if_needed(body: Dict[str, Any]) -> Dict[str, Any]:
    """
    Support CloudEvent structured mode bodies:
      {
        "specversion": "...",
        "type": "...",
        "source": "...",
        "id": "...",
        "time": "...",
        "data": { ... }   # may contain Pub/Sub envelope or direct payload
      }
    """
    data = body.get("data")
    if isinstance(data, dict):
        # If this is a CloudEvent wrapper, use data as the "working body"
        # (works for both direct payload and Pub/Sub envelope under data.message)
        return data
    return body


def _decode_pubsub_data_to_job(message: Dict[str, Any]) -> Tuple[Optional[Dict[str, Any]], str]:
    b64 = message.get("data")
    if not b64:
        return None, "Pub/Sub envelope missing message.data"

    try:
        decoded_bytes = base64.b64decode(b64)
    except Exception as e:
        return None, f"Failed base64 decode: {e}"

    try:
        decoded_text = decoded_bytes.decode("utf-8")
    except Exception as e:
        return None, f"Decoded bytes are not UTF-8: {e}"

    try:
        job = json.loads(decoded_text)
        if not isinstance(job, dict):
            return None, "Decoded Pub/Sub message is not a JSON object"
        return job, ""
    except Exception as e:
        return None, f"Failed to parse decoded JSON: {e}"


def _extract_job_from_http_request(request: Request) -> Tuple[Optional[Dict[str, Any]], str, bool]:
    """
    Supports:
    A) Pub/Sub push JSON envelope:
       { "message": { "data": "<base64-json>" , ... }, "subscription": "..." }

    B) Eventarc / CloudEvent structured mode:
       { ..., "data": { "message": { "data": "<base64-json>" } } }   OR
       { ..., "data": { "docId": "...", "canonicalGcsUri": "..." } }

    C) Direct JSON job (manual curl/test):
       { "docId": "...", "canonicalGcsUri": "...", ... }

    Returns: (job, err, is_pubsub_like)
    """
    body, err = _parse_json_body(request)
    if err:
        return None, err, _is_pubsub_like_request(request, None)

    # unwrap CloudEvent structured body if applicable
    working = _unwrap_cloudevent_if_needed(body)

    is_pubsub_like = _is_pubsub_like_request(request, body) or _is_pubsub_like_request(request, working)

    # Pub/Sub envelope (standard)
    if isinstance(working, dict) and isinstance(working.get("message"), dict):
        job, derr = _decode_pubsub_data_to_job(working["message"])
        if derr:
            return None, derr, True
        return job, "", True

    # Direct job
    if isinstance(working, dict) and "docId" in working and "canonicalGcsUri" in working:
        return working, "", is_pubsub_like

    return None, "Unrecognized payload format", is_pubsub_like


def _compute_page_scores_from_document_layout(doc_like: dict):
    """
    Best-effort heuristic score based on DocumentAI documentLayout.blocks.
    Keep as-is but make it safe.
    """
    layout = doc_like.get("documentLayout") or {}
    blocks = layout.get("blocks") or []

    pages = {}
    for b in blocks:
        if not isinstance(b, dict):
            continue
        span = b.get("pageSpan") or {}
        p = span.get("pageStart")
        if not p:
            continue
        txt = ((b.get("textBlock") or {}).get("text")) or ""
        pages.setdefault(p, {"page": p, "textLen": 0, "numBlocks": 0})
        pages[p]["textLen"] += len(txt)
        pages[p]["numBlocks"] += 1

    results = []
    for p in sorted(pages.keys()):
        tl = pages[p]["textLen"]
        nb = pages[p]["numBlocks"]
        s_len = min(1.0, tl / 3000.0)
        s_blk = min(1.0, nb / 10.0)
        score = round(s_len * s_blk, 4)

        flags = []
        if tl < 50:
            flags.append("empty_or_near_empty")

        results.append({
            "page": p,
            "score": score,
            "textLen": tl,
            "numBlocks": nb,
            "flags": flags,
        })

    if not results:
        return [], 0.0

    doc_score = round(sum(r["score"] for r in results) / len(results), 4)
    return results, doc_score


def _write_json_to_gcs(prefix: str, obj_name: str, payload: dict):
    b = storage_client.bucket(DOCAI_OUT_BUCKET)
    blob = b.blob(f"{prefix}/{obj_name}")
    blob.upload_from_string(
        json.dumps(payload, ensure_ascii=False),
        content_type="application/json",
    )


def _fail_firestore(doc_id: str, status: str, error: str):
    try:
        fs.collection("documents").document(doc_id).set({
            "status": status,
            "docai": {
                "error": error,
                "updatedAtUtc": _utc_ts(),
                "processor": DOCAI_PROCESSOR,
            },
            "updatedAt": firestore.SERVER_TIMESTAMP,
        }, merge=True)
    except Exception:
        # Don't crash on secondary failure
        print("WARN: failed to update firestore failure state")


def _process_job(job: Dict[str, Any]) -> Tuple[str, int]:
    doc_id = job["docId"]
    canonical_gcs_uri = job["canonicalGcsUri"]

    # Firestore mark running
    doc_ref = fs.collection("documents").document(doc_id)
    doc_ref.set({
        "status": "DOCAI_RUNNING",
        "docai": {
            "processor": DOCAI_PROCESSOR,
            "startedAtUtc": _utc_ts(),
        },
        "updatedAt": firestore.SERVER_TIMESTAMP
    }, merge=True)

    # DocAI call
    client = documentai.DocumentProcessorServiceClient()

    # Keep your current request style (works with GCS URI)
    req = documentai.ProcessRequest(
        name=DOCAI_PROCESSOR,
        gcs_document=documentai.GcsDocument(
            gcs_uri=canonical_gcs_uri,
            mime_type="application/pdf",
        ),
    )

    result = client.process_document(request=req)

    doc_dict = MessageToDict(result.document._pb, preserving_proto_field_name=True)

    pages, doc_score = _compute_page_scores_from_document_layout(doc_dict)

    ts = _utc_ts()
    prefix = f"{doc_id}/{ts}"

    # Write outputs
    _write_json_to_gcs(prefix, "docai_document.json", doc_dict)

    manifest = {
        "docId": doc_id,
        "canonicalGcsUri": canonical_gcs_uri,
        "rawGcsUri": job.get("rawGcsUri"),
        "processor": DOCAI_PROCESSOR,
        "createdAtUtc": ts,
        "docQualityScore": doc_score,
    }
    _write_json_to_gcs(prefix, "manifest.json", manifest)

    # Firestore mark done
    doc_ref.set({
        "status": "DOCAI_DONE",
        "docai": {
            "outputPrefix": f"gs://{DOCAI_OUT_BUCKET}/{prefix}/",
            "docQualityScore": doc_score,
            "pages": pages,
            "processor": DOCAI_PROCESSOR,
            "finishedAtUtc": _utc_ts(),
        },
        "updatedAt": firestore.SERVER_TIMESTAMP,
    }, merge=True)

    # Publish results message (downstream)
    out_msg = {
        "docId": doc_id,
        "docaiOutputPrefix": f"gs://{DOCAI_OUT_BUCKET}/{prefix}/",
        "docQualityScore": doc_score,
        "canonicalGcsUri": canonical_gcs_uri,
    }
    publisher.publish(topic_path, json.dumps(out_msg).encode("utf-8"))

    print(f"OK docId={doc_id} docScore={doc_score} pages={len(pages)} out=gs://{DOCAI_OUT_BUCKET}/{prefix}/")
    return ("ok", 200)


@functions_framework.http
def docai_runner(request: Request):
    """
    IMPORTANT behavior for Pub/Sub PUSH:
      - On SUCCESS -> return 200 so Pub/Sub stops retrying.
      - On MALFORMED/UNRECOGNIZED payload -> return 200 (ACK) to prevent poison-message retry loops.
      - On REAL processing failure -> return 500 so Pub/Sub retries.

    For direct/manual requests (not Pub/Sub), we return 400 on bad payload.
    """
    try:
        if request.method == "GET":
            return ("OK", 200)

        job, err, is_pubsub_like = _extract_job_from_http_request(request)

        if err:
            msg = f"BAD REQUEST: {err}"
            print(msg)

            # ACK malformed Pub/Sub / CloudEvent deliveries to stop retries
            if is_pubsub_like:
                return (msg, 200)

            # For direct calls, reflect as 400
            return (msg, 400)

        # Validate required fields
        if "docId" not in job or "canonicalGcsUri" not in job:
            msg = "BAD REQUEST: missing docId/canonicalGcsUri"
            print(msg)
            if is_pubsub_like:
                return (msg, 200)
            return (msg, 400)

        print(f"Received job docId={job.get('docId')} canonical={job.get('canonicalGcsUri')}")
        return _process_job(job)

    except Exception as e:
        # This is a real failure: return 500 so Pub/Sub retries
        print(f"ERROR processing job: {e}")
        print(traceback.format_exc())

        # Best-effort: if we can infer docId, mark failed
        try:
            body, _ = _parse_json_body(request)
            body = body or {}
            work = _unwrap_cloudevent_if_needed(body) if isinstance(body, dict) else {}
            # try direct
            doc_id = work.get("docId")
            if not doc_id and isinstance(work.get("message"), dict):
                j, _ = _decode_pubsub_data_to_job(work["message"])
                if isinstance(j, dict):
                    doc_id = j.get("docId")
            if doc_id:
                _fail_firestore(doc_id, "DOCAI_FAILED", str(e))
        except Exception:
            pass

        return ("internal error", 500)
